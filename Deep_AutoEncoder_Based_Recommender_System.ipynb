{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deep AutoEncoder Based Recommender System"
      ],
      "metadata": {
        "id": "9KeCwyxAc5Fp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKf0CV4j6gkc",
        "outputId": "41d1ed1e-e64d-4901-b0cc-5e499d2757f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              userId        movieId         rating     timestamp\n",
            "count  100836.000000  100836.000000  100836.000000  1.008360e+05\n",
            "mean      326.127564   19435.295718       3.501557  1.205946e+09\n",
            "std       182.618491   35530.987199       1.042529  2.162610e+08\n",
            "min         1.000000       1.000000       0.500000  8.281246e+08\n",
            "25%       177.000000    1199.000000       3.000000  1.019124e+09\n",
            "50%       325.000000    2991.000000       3.500000  1.186087e+09\n",
            "75%       477.000000    8122.000000       4.000000  1.435994e+09\n",
            "max       610.000000  193609.000000       5.000000  1.537799e+09\n",
            "0 Completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-fde1f219bb0b>:53: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  train_ratings = pd.concat([train_ratings, pd.DataFrame([{'userId': rat['userId'].iloc[i], 'movieId': rat['movieId'].iloc[i], 'rating': rat['rating'].iloc[i]}])])\n",
            "<ipython-input-5-fde1f219bb0b>:54: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  validation_ratings = pd.concat([validation_ratings, pd.DataFrame([{'userId': rat['userId'].iloc[i], 'movieId': rat['movieId'].iloc[i], 'rating': rat['rating'].iloc[i]}])])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000 Completed\n",
            "20000 Completed\n",
            "30000 Completed\n",
            "40000 Completed\n",
            "50000 Completed\n",
            "60000 Completed\n",
            "70000 Completed\n",
            "80000 Completed\n",
            "90000 Completed\n",
            "100000 Completed\n",
            "98819\n",
            "100836\n",
            "9559\n",
            "9608\n",
            "Train Users:  595\n",
            "Validation Users:  595\n",
            "Train Movies:  9559\n",
            "Validation Movies:  9559\n",
            "movieId  1       2       3       4       5       6       7       8       \\\n",
            "userId                                                                    \n",
            "1           4.0     0.0     4.0     0.0     0.0     4.0     0.0     0.0   \n",
            "2           0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "3           0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "4           0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "5           4.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "6           0.0     4.0     5.0     3.0     5.0     4.0     4.0     3.0   \n",
            "7           4.5     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "8           0.0     4.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "9           0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "10          0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "\n",
            "movieId  9       10      ...  184349  184471  184791  185029  185031  185135  \\\n",
            "userId                   ...                                                   \n",
            "1           0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "2           0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "3           0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "4           0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "5           0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "6           0.0     3.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "7           0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "8           0.0     2.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "9           0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "10          0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "\n",
            "movieId  185435  187593  187595  189547  \n",
            "userId                                   \n",
            "1           0.0     0.0     0.0     0.0  \n",
            "2           0.0     0.0     0.0     0.0  \n",
            "3           0.0     0.0     0.0     0.0  \n",
            "4           0.0     0.0     0.0     0.0  \n",
            "5           0.0     0.0     0.0     0.0  \n",
            "6           0.0     0.0     0.0     0.0  \n",
            "7           0.0     0.0     0.0     0.0  \n",
            "8           0.0     0.0     0.0     0.0  \n",
            "9           0.0     0.0     0.0     0.0  \n",
            "10          0.0     0.0     0.0     0.0  \n",
            "\n",
            "[10 rows x 9559 columns]\n",
            "movieId  1       2       3       4       5       6       7       8       \\\n",
            "userId                                                                    \n",
            "1           4.0     0.0     4.0     0.0     0.0     4.0     0.0     0.0   \n",
            "2           0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "3           0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "4           0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "5           4.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "6           0.0     4.0     5.0     3.0     5.0     4.0     4.0     3.0   \n",
            "7           4.5     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "8           0.0     4.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "9           0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "10          0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "\n",
            "movieId  9       10      ...  184349  184471  184791  185029  185031  185135  \\\n",
            "userId                   ...                                                   \n",
            "1           0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "2           0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "3           0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "4           0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "5           0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "6           0.0     3.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "7           0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "8           0.0     2.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "9           0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "10          0.0     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "\n",
            "movieId  185435  187593  187595  189547  \n",
            "userId                                   \n",
            "1           0.0     0.0     0.0     0.0  \n",
            "2           0.0     0.0     0.0     0.0  \n",
            "3           0.0     0.0     0.0     0.0  \n",
            "4           0.0     0.0     0.0     0.0  \n",
            "5           0.0     0.0     0.0     0.0  \n",
            "6           0.0     0.0     0.0     0.0  \n",
            "7           0.0     0.0     0.0     0.0  \n",
            "8           0.0     0.0     0.0     0.0  \n",
            "9           0.0     0.0     0.0     0.0  \n",
            "10          0.0     0.0     0.0     0.0  \n",
            "\n",
            "[10 rows x 9559 columns]\n",
            "Epoch 1, Loss: 10.766819095611572\n",
            "Epoch 2, Loss: 8.17040719985962\n",
            "Epoch 3, Loss: 7.124985980987549\n",
            "Epoch 4, Loss: 6.208682441711426\n",
            "Epoch 5, Loss: 5.572541999816894\n",
            "Epoch 6, Loss: 3.5109955787658693\n",
            "Epoch 7, Loss: 3.096630907058716\n",
            "Epoch 8, Loss: 2.58579421043396\n",
            "Epoch 9, Loss: 2.452072811126709\n",
            "Epoch 10, Loss: 1.9424251556396483\n",
            "Epoch 11, Loss: 1.7388405323028564\n",
            "Epoch 12, Loss: 1.607527756690979\n",
            "Epoch 13, Loss: 1.4484813690185547\n",
            "Epoch 14, Loss: 1.2949333906173706\n",
            "Epoch 15, Loss: 1.1642747163772582\n",
            "Epoch 16, Loss: 1.1040343761444091\n",
            "Epoch 17, Loss: 1.053538429737091\n",
            "Epoch 18, Loss: 0.9083605170249939\n",
            "Epoch 19, Loss: 0.8623626470565796\n",
            "Epoch 20, Loss: 0.7867071509361268\n",
            "Epoch 21, Loss: 0.7508268833160401\n",
            "Epoch 22, Loss: 0.7270700216293335\n",
            "Epoch 23, Loss: 0.7015454769134521\n",
            "Epoch 24, Loss: 0.6869238376617431\n",
            "Epoch 25, Loss: 0.6745378732681274\n",
            "Epoch 26, Loss: 0.6621556282043457\n",
            "Epoch 27, Loss: 0.6677278041839599\n",
            "Epoch 28, Loss: 0.6552232503890991\n",
            "Epoch 29, Loss: 0.6598787426948547\n",
            "Epoch 30, Loss: 0.6456579208374024\n",
            "Epoch 31, Loss: 0.6412040829658509\n",
            "Epoch 32, Loss: 0.638126528263092\n",
            "Epoch 33, Loss: 0.6458487987518311\n",
            "Epoch 34, Loss: 0.6286562204360961\n",
            "Epoch 35, Loss: 0.6373852372169495\n",
            "Epoch 36, Loss: 0.6288082122802734\n",
            "Epoch 37, Loss: 0.6230728030204773\n",
            "Epoch 38, Loss: 0.6258058667182922\n",
            "Epoch 39, Loss: 0.6211102128028869\n",
            "Epoch 40, Loss: 0.6139011263847352\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Created on Nov 05, 2023\n",
        "\n",
        "@author: Gaurav Bharatavalli Rangaswamy\n",
        "'''\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class TrainDataset(Dataset):\n",
        "    def __init__(self, train_file, transform=None):\n",
        "        self.data = pd.read_csv(train_file)\n",
        "        self.data = self.data.iloc[:, 1:]  # Drop the first column\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        user_vector = self.data.iloc[ind].values.astype(np.float32)\n",
        "        user_vector = torch.FloatTensor(user_vector)  # Convert directly to tensor\n",
        "        return user_vector\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, test_file, transform=None):\n",
        "        self.data = pd.read_csv(test_file)\n",
        "        self.data = self.data.iloc[:, 1:]  # Drop the first column\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        user_vector = self.data.iloc[ind].values.astype(np.float32)\n",
        "        user_vector = torch.FloatTensor(user_vector)  # Convert directly to tensor\n",
        "        return user_vector\n",
        "\n",
        "\n",
        "def prepare_train_validation_movielens_step1():\n",
        "    rat = pd.read_csv('/content/ratings.csv')\n",
        "    mov = pd.read_csv('/content/movies.csv')\n",
        "    df_combined = pd.merge(rat, mov, on='movieId')\n",
        "    print(rat.describe())\n",
        "    ts = rat['timestamp'].quantile(0.98)\n",
        "    train_ratings = pd.DataFrame(columns=['userId', 'movieId', 'rating'])\n",
        "    validation_ratings = pd.DataFrame(columns=['userId', 'movieId', 'rating'])\n",
        "    for i in range(len(rat)):\n",
        "        if rat['timestamp'].iloc[i] <= ts:\n",
        "            train_ratings = pd.concat([train_ratings, pd.DataFrame([{'userId': rat['userId'].iloc[i], 'movieId': rat['movieId'].iloc[i], 'rating': rat['rating'].iloc[i]}])])\n",
        "            validation_ratings = pd.concat([validation_ratings, pd.DataFrame([{'userId': rat['userId'].iloc[i], 'movieId': rat['movieId'].iloc[i], 'rating': rat['rating'].iloc[i]}])])\n",
        "        else:\n",
        "            validation_ratings = pd.concat([validation_ratings, pd.DataFrame([{'userId': rat['userId'].iloc[i], 'movieId': rat['movieId'].iloc[i], 'rating': rat['rating'].iloc[i]}])])\n",
        "        if i % 10000 == 0:\n",
        "            print(i, \"Completed\")\n",
        "    print(len(train_ratings))\n",
        "    print(len(validation_ratings))\n",
        "    # Remove users in validation set those are not present in Training Set\n",
        "    train_users = train_ratings['userId'].unique()\n",
        "    users_not_in_train_set = []\n",
        "\n",
        "    for i in range(1, 611):\n",
        "        if i not in train_users:\n",
        "            users_not_in_train_set.append(i)\n",
        "\n",
        "    for i in users_not_in_train_set:\n",
        "        validation_ratings = validation_ratings[validation_ratings['userId'] != i]\n",
        "\n",
        "    validation_ratings.reset_index(drop=True)\n",
        "\n",
        "    print(len(train_ratings['movieId'].unique()))\n",
        "    print(len(validation_ratings['movieId'].unique()))\n",
        "    # Remove Movies that are not in the Train Set\n",
        "    validation_movies = validation_ratings['movieId'].unique()\n",
        "    train_movies = train_ratings['movieId'].unique()\n",
        "    movies_not_in_train_set = []\n",
        "\n",
        "    for i in validation_movies:\n",
        "        if i not in train_movies:\n",
        "            movies_not_in_train_set.append(i)\n",
        "\n",
        "    for i in movies_not_in_train_set:\n",
        "        validation_ratings = validation_ratings[validation_ratings['movieId'] != i]\n",
        "\n",
        "    validation_ratings.reset_index(drop=True)\n",
        "    print('Train Users: ', train_ratings['userId'].nunique())\n",
        "    print('Validation Users: ', validation_ratings['userId'].nunique())\n",
        "    print('Train Movies: ', train_ratings['movieId'].nunique())\n",
        "    print('Validation Movies: ', validation_ratings['movieId'].nunique())\n",
        "    train_ratings.to_csv(\"/content/train_ratings.csv\")\n",
        "    validation_ratings.to_csv(\"/content/validation_ratings.csv\")\n",
        "\n",
        "def prepare_traintest_movielens_step2():\n",
        "    tr_ratings = pd.read_csv('/content/train_ratings.csv')\n",
        "    val_ratings = pd.read_csv('/content/validation_ratings.csv')\n",
        "    train_dataset = tr_ratings.pivot_table(index='userId', columns='movieId', values='rating')\n",
        "    train_dataset.fillna(0, inplace=True)\n",
        "    print(train_dataset.head(10))\n",
        "    test_dataset = val_ratings.pivot_table(index='userId', columns='movieId', values='rating')\n",
        "    test_dataset.fillna(0, inplace=True)\n",
        "    print(test_dataset.head(10))\n",
        "    train_dataset.to_csv('/content/train.csv')\n",
        "    test_dataset.to_csv('/content/test.csv')\n",
        "\n",
        "def get_traintestloaders():\n",
        "    train_dat = TrainDataset('/content/train.csv')\n",
        "    test_dat = TestDataset('/content/test.csv')\n",
        "    train_loader = DataLoader(dataset=train_dat, batch_size=128, shuffle=True, num_workers=1)\n",
        "    test_loader = DataLoader(dataset=test_dat, batch_size=128, shuffle=True, num_workers=1)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "class MSELoss_with_Mask(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MSELoss_with_Mask, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        # Masking into a vector of 1's and 0's.\n",
        "        mask = (targets != 0).float()\n",
        "        # Actual number of ratings.\n",
        "        number_ratings = torch.max(torch.sum(mask), torch.tensor(1.0).cuda())\n",
        "        error = torch.sum(mask * (targets - inputs) ** 2)\n",
        "        loss = error / number_ratings\n",
        "        return loss\n",
        "\n",
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self, encoder_layers_sizes, activation='ReLU'):\n",
        "        super(AutoEncoder, self).__init__()\n",
        "        # Encoder layers\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(encoder_layers_sizes[0], encoder_layers_sizes[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(encoder_layers_sizes[1], encoder_layers_sizes[2]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(encoder_layers_sizes[2], encoder_layers_sizes[3]),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Decoder layers\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(encoder_layers_sizes[3], encoder_layers_sizes[2]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(encoder_layers_sizes[2], encoder_layers_sizes[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(encoder_layers_sizes[1], encoder_layers_sizes[0])\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "def train(model, criterion, optimizer, train_loader, test_loader, num_epochs=50):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = 0.0\n",
        "        for data in train_loader:\n",
        "            inputs = data.cuda()\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, inputs)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Loss: {train_loss/len(train_loader)}')\n",
        "\n",
        "def main():\n",
        "    prepare_train_validation_movielens_step1()\n",
        "    prepare_traintest_movielens_step2()\n",
        "    train_loader, test_loader = get_traintestloaders()\n",
        "    encoder_layers_sizes = [9559, 512, 512, 1024]\n",
        "    model = AutoEncoder(encoder_layers_sizes)\n",
        "    model = model.cuda()\n",
        "    criterion = MSELoss_with_Mask().cuda()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    train(model, criterion, optimizer, train_loader, test_loader, 40)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ]
}